# Groq Model Limits
# Auto-generated by fetch_groq_limits.py
# DO NOT EDIT MANUALLY - Run fetch_groq_limits.py to update
#
# TPM (tokens per minute) limits are based on on-demand tier and may vary by service tier.
# These are the limits we've observed from error messages and documentation.

GROQ_MODEL_LIMITS = {
    "allam-2-7b": {
        "context_window": 4096,
        "max_input_tokens": 0,
        "max_completion_tokens": 4096,
        "tpm_limit": 6000,  # On-demand tier
    },
    "groq/compound": {
        "context_window": 131072,
        "max_input_tokens": 122880,
        "max_completion_tokens": 8192,
        "tpm_limit": 6000,  # On-demand tier
    },
    "groq/compound-mini": {
        "context_window": 131072,
        "max_input_tokens": 122880,
        "max_completion_tokens": 8192,
        "tpm_limit": 6000,  # On-demand tier
    },
    "llama-3.1-8b-instant": {
        "context_window": 131072,
        "max_input_tokens": 0,
        "max_completion_tokens": 131072,
        "tpm_limit": 6000,  # On-demand tier
    },
    "llama-3.3-70b-versatile": {
        "context_window": 131072,
        "max_input_tokens": 98304,
        "max_completion_tokens": 32768,
        "tpm_limit": 12000,  # On-demand tier (higher limit for this model)
    },
    "meta-llama/llama-4-maverick-17b-128e-instruct": {
        "context_window": 131072,
        "max_input_tokens": 122880,
        "max_completion_tokens": 8192,
        "tpm_limit": 6000,  # On-demand tier
    },
    "meta-llama/llama-4-scout-17b-16e-instruct": {
        "context_window": 131072,
        "max_input_tokens": 122880,
        "max_completion_tokens": 8192,
        "tpm_limit": 6000,  # On-demand tier
    },
    "moonshotai/kimi-k2-instruct": {
        "context_window": 131072,
        "max_input_tokens": 114688,
        "max_completion_tokens": 16384,
        "tpm_limit": 6000,  # On-demand tier
    },
    "moonshotai/kimi-k2-instruct-0905": {
        "context_window": 262144,
        "max_input_tokens": 245760,
        "max_completion_tokens": 16384,
        "tpm_limit": 6000,  # On-demand tier
    },
    "openai/gpt-oss-120b": {
        "context_window": 131072,
        "max_input_tokens": 65536,
        "max_completion_tokens": 65536,
        "tpm_limit": 6000,  # On-demand tier
    },
    "openai/gpt-oss-20b": {
        "context_window": 131072,
        "max_input_tokens": 65536,
        "max_completion_tokens": 65536,
        "tpm_limit": 6000,  # On-demand tier
    },
    "qwen/qwen3-32b": {
        "context_window": 131072,
        "max_input_tokens": 90112,
        "max_completion_tokens": 40960,
        "tpm_limit": 6000,  # On-demand tier
    },
}
